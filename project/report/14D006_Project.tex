\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage[bb=boondox]{mathalfa}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{bm}
\usepackage{graphicx}
\usepackage{dsfont}
\graphicspath{ {images/} }

%SetFonts

%SetFonts

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{
\normalfont \normalsize
\textsc{14D006 Stochastic Models and Optimization} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Play Selection in American Football\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Daniel Bestard, Michael Cameron, Hans-Peter H{\"o}llwirth, Akhil Lohia} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle

%%%%%%%%
% Motivation %
%%%%%%%%
\section{Motivation}
In the following sections we assume some basic understanding of the American football rules. We use a dynamic programming algorithm to develop an optimal policy for play choice in an American football game. In this context, the policies refer to the different plays an American football team can make at each game situation. At every field postition, the aim of the team is to choose among different possible options, such as run, pass, kick and punt, which will be explained in more detail in the next section, the one which increases the expected reward the most. The reward is the difference in  number of points recieved at the end of our possesion and the anticipated points of the opposition from our final field position. Each option gives the team different advantages at different risks. Dynamic programming works backwards to allow us, given any field position, to make the optimal policy choice. \\
In this problem, there is a large number of possible states, 15250, which leads to computational difficulties. To solve this, we use simulations to replace the exact reward-to-go function with an approximation that makes the optimisation problem feasible to solve. Approximate Policy Iteration (API) and Optimistic Policy Iteration (OPI) to calculate this approximation.


%%%%%%%%%%%%
% The Football Model %
%%%%%%%%%%%%
\section{The Football Model}
In our American football model we consider the score of one offense drive and add to it the expected points gained by the opposing team from our final field position. More concretely, we want to maximize the expected difference between our drive-score and the opposing team's responding drive-score (which is simply a function of our final field position). We now introduce the elements used by the dynamic programming algorithms to solve the optimisation problem. \\\\
The state of the system $i \in S$ is described by a vector of 3 quantities: $i = [x,y,d]$:
\begin{itemize}
\item x = the number of yards to the opposition goal line (discrete value between 1 and 100)
\item y = the number of yards to go until the next first down (discrete value between 1 and 100)
\item d = down number ($\in \{1,2,3,4\}$)
\end{itemize}

\begin{figure}[ht!]
\centering
\includegraphics[width=155mm]{../images/field.png}
\caption{Illustration of state}
\end{figure}

At each state, the team can choose from one of 4 play options (actions) $u \in U$ with $U=\{R, P, U, K\}$. Each play is described using probablility distributions, to model the yards gained. For this section we use the models set out in Bertsekas and Patek's article:
\begin{itemize}
\item (R)un: moves $D_p - 2$ yards forward with $D_p \sim \text{Poisson}(6)$, with probability 0.05 of a fumble.
\item (P)ass: moves $D_p - 2$ yards forward with $D_p \sim \text{Poisson}(12)$, with probability 0.05 of an interception, 0.45 of incompletion, 0.05 of quarterback sack and 0.45 of completion. 
\item P(u)nt: moves $6 D_p  + 6$ yards forward with $D_p \sim \text{Poisson}(10)$ 
\item (K)ick: successful with probability $\max(0, .95 - .95x/60)$
\end{itemize}
The set of state-action pairs determine the stationary policy $\mu$. We look to choose the policy at any given state to maximise the expected reward. The reward of the drive is determined by the final state transition.:
\begin{itemize}
\item Touchdown: 6.8 points (from run or pass) 
\item Opponent's touchdown: -6.8 points (from run or pass)
\item Safety: -2.0 points (from pass or run)
\item Field goal: 3.0 points (from kick)
\item No score due to fumble (from run), interception (from pass), missed 4th down (from pass or run), missed field-goal (from kick) or punt
\end{itemize}
Once the inputs of the dynamic programming algorithm have been defined, the next step is construct the algorithm itself.

%%%%%%%%%%%%%%%%%%%%
% Dynamic Programming Formulation %
%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Dynamic Programming Formulation}
In this model of the football problem, the solution can be computed directly. However, the problem can quickly become computationally infeasible when introducing other elements of the game, such as time or half yard intervals. 
\subsection{Optimal Solution}
Our original problem has 15250 states. The rules of our model are such that at first down there is a unique value of y associated with each value of x. Note, that the number of possible combinations is much larger than the 15250 that we claim. There is the additional constraint that it is impossible to have the number of yards to next down greater than yards to the goal line, that is, we cannot have $y>x$. \\

This is computationally tractable, so we can apply the dynamic programming algorithm. 
$$
\mu^{k}(i) = arg\max\limits_{u \in U} \Big[ \sum\limits_{j \in S} p_{ij}(u)(g(i,u,j) +  J^{\mu^{k-1}}(j))\Big] \quad \forall i \in S
$$
This algorithm chooses the policy u  which maximises the expected reward. In the formula above, $i$ represents the state we are currently in and $j$ is the state we move to. $p_{ij}(u)$ are the transition probabilities, giving the probability from moving from state $i$ to state $j$ under action $u$. The reward function is $g(i,u,j)$, which is 0 in every stage except for the terminal state. This is because we only gain rewards after scoring or losing the ball. While $J^{\mu^{k-1}}(j)$ is the reward-to-go  function from the state $j$. \\

Using this exact method we obtain an optimal policy for any state of the 4 downs. These optimal policies can been in Figure 1,1 in Bertsekas and Patek. As a benchmark, using this policy from the state $(x,y,d)=(80,10,1)$ and running simulations with the optimal policy, the expected net reward is -0.9449 points. This means that if we start each drive in this state, we are expected to lose the game!

\subsection{Approximation with Neuro-Dynamic Programming}

%%%%%%%%
% Simulation %
%%%%%%%%
\section{Simulation}
The approximations of the reward-to-go function $J_{u_k}(i^*)$ are obtained from simulated sample trajectories. The simulation routine takes as inputs a policy $\mu$ and a starting state $i^*$ and then generates $N$ sample drives. Each generated sample drive represents a realization of the following probabilistic model (probabilities in brackets):
\begin{itemize}
\item (R)un attempts result in either
\begin{itemize}
\item (0.05) fumble
\item (0.95) run with movement $D_r -2$ with $D_r \sim \text{Poisson}(6)$
\end{itemize}
\item (P)ass attempts result in either
\begin{itemize}
\item (0.05) pass interception with movement $D_p -2$ with $D_p \sim \text{Poisson}(12)$
\item (0.05) sack with movement $D_s$ with $D_s \sim \text{Poisson}(6)$
\item (0.45) pass incomplete
\item (0.45) pass complete with movement $D_p -2$ with $D_p \sim \text{Poisson}(12)$
\end{itemize}
\item P(U)nt attempts always result in turn-over with movement $6 D_p + 6$ with $D_p \sim \text{Poisson}(10)$ 
\item (K)ick attempts result in either
\begin{itemize}
\item (max(0,(0.95-0.95x/60)) successful field goal
\item (otherwise) missed field goal
\end{itemize}
\end{itemize}

The drive terminates in any of the following events:

The simulation returns $N$ sample drives. Each drive $l$ is described by its status sequence $(i_t^l)$ for $t=1,...,T^l$ and reward $g_{T^i}^l$.

\subsection{Expected Reward}
Given a set of $N$ simulated sample trajectories for a specific policy $\mu$, we can estimate the expected reward of this policy from a starting state $i^*$:
$$
\widetilde{J}_{u_k}(i^*) = \frac{1}{N}\sum_{l=1}^N g_{T^i}^l
$$
where $g_{T^i}^i$ denotes the reward of the $i^{th}$ sample trajectory with drive length $T^i$.

\subsection{Heuristic Benchmark}
Rhe simulations and the expected reward function can be used to yield a heuristic benchmark for the optimal play-selection policy. We use the suggested heuristic policies from the paper in order to establish the correctness of the simulation algorithm. In particular, we consider the following heuristic policies:
\begin{enumerate}
\item If $d=1$ (first down): (\textbf{P})ass

\item If $d=2$ (second down): 
\begin{enumerate}
\item If $y<3$ (less than 3 yards to next first down): (\textbf{R})un
\item If $y\geq3$ (3 or more yards to next first down): (\textbf{P})ass
\end{enumerate}

\item If $d=3$ (third down): 
\begin{enumerate}
\item If $x<41$ (less than 41 yards to goal):
\begin{enumerate}
\item If $y<3$ (less than 3 yards to next first down): (\textbf{P})ass or (R)un
\item If $y\geq3$ (3 or more yards to next first down): (\textbf{P})ass or (R)un
\end{enumerate}
\item If $x \geq 41$ (less than 41 yards to goal):
\begin{enumerate}
\item If $y<3$ (less than 3 yards to next first down): (P)ass or (\textbf{R})un
\item If $y\geq3$ (3 or more yards to next first down): (\textbf{P})ass or (R)un
\end{enumerate}
\end{enumerate}

\item If $d=4$ (forth down): 
\begin{enumerate}
\item If $x<41$ (less than 41 yards to goal):
\begin{enumerate}
\item If $y<3$ (less than 3 yards to next first down): (P)ass or (\textbf{R})un or (K)ick
\item If $y\geq3$ (3 or more yards to next first down): (P)ass or (R)un or (\textbf{K})ick
\end{enumerate}
\item If $x \geq 41$ (less than 41 yards to goal):
\begin{enumerate}
\item If $y<3$ (less than 3 yards to next first down): (P)ass or (\textbf{R})un or P(U)nt
\item If $y\geq3$ (3 or more yards to next first down): (P)ass or (R)un or P(\textbf{U})nt
\end{enumerate}
\end{enumerate}

\end{enumerate}

We estimated the expected reward for each of the $2^4*3^4 = 1296$ heuristic policy combinations from starting position $i^* = (x=80, y=10, d=1)$ (one of the most likely starting positions in football) and found the best heuristic expected reward-to-go $J_{\mu}(i^*) = -1.26$. The associated policy to this reward is highlighted in bold. Note that the reward-to-go matches the heuristic result of the underlying paper, thus establishing the correctness of the simulation algorithm.

%%%%%%%%%
% API and OPI %
%%%%%%%%%
\section{Approximate and Optimistic Policy Iteration}
The aim now is to approximate the reward-to-go function to be used in the policy update algorithm, We have two different approaches to this problem, API and OPI. In API, we run many drive simulations over few different policies. While in contrast, for OPI, we run much fewer simulations, but over much more policy choices.

We now describe in detail how the algorithms API and OPI are implemented in practice. Let $N_{p}$, $N_{e}$ , $N_{s}$ and $N_{t}$ denote the parameters of the algorithm which are set by the user in advance. Even thought the definition of these parameters can be inferred from the algorithm, it is more clear to define them separately:
\begin{itemize}
    \item $N_{p}$: if, for example, $N_{p}=20$, then in every 20th iteration we compute the expected reward-to-go.
    \item $N_{e}$: number of sample trajectories that we use to compute the estimate of the reward-to-go function.
    \item $N_{s}$: number of sample trajectories in the in training set $D_{k}$ (see later).
    \item $N_{t}$: number of times we train the neural network.
\end{itemize}
The implementation of the algorithm is as follows:
\begin{enumerate}
    \item Start an initial policy $\mu_{0}$.
    \item Given $\mu_{k}$, if $k \in \{j \cdot N_{p} | j=0,1,2,â¦\}$, then generate $N_{e}$ sample trajectories, each starting from $i^{*}$, to obtain an estimate of $J_{\mu_{k}}(i^{*})$
    \item Given the probabilistic rule for picking initial conditions, generate $N_{s}$ sample trajectories and record $D_{k}$, where the last element is simply an object that stores the reward and the sequence of the vector of states. $D_{k}$ is created because we use its elements in the next step of the algorithm.
    \item We fit the neural networks to estimate the reward-to-go function and save the object in a variable that we call $r^{k}$ which uses the elements of $D_{k}$. The neural networks cycle through the data $N_{t}$ times.
    \item Compute a new policy $\mu_{k+1} := G(r^{k})$, where $G$ is the âgreedyâ operator which chooses the actions at each state that are best with respect to the reward-to-go approximation given by $r^{k}$.
\end{enumerate}

\subsection{Multilayer Perceptron (MLP)}

As stated ealier in the report, we can replace the exact values of the reward-to-go function with approximates. To obtain these estimates we can use a trained neural network. The network has 2 inputs $x$ and $y$, one hidden layer with 20 nodes, and is trained on the exact output $J$. We construct 4 of these networks, one for each possible down. For each of these neural networks we input the feature vector $\xi^{\Sigma}(i) = (\sigma_{d}^{x} x_{i}, \sigma_{d}^{y} y_{i})$, In the football example we set $\sigma_{d}^{x} = \sigma_{d}^{y} = 0.01$ to ensure that all elements of $\xi^{\Sigma}(i)$ are in $[0,1]$. This now allows us to get an estimate for the reward-to-go function from any state, by inputting $x$ and $y$ in the feature vectors into the correct neural network depending on the down. 


\begin{figure}[ht!]
\centering
\includegraphics[width=155mm]{../images/neuralnet.png}
\caption{Architecture of neural network with one hidden layer ($R=20$ nodes)}
\end{figure}

\subsection{Policy Update}

%%%%%%
% Results %
%%%%%%
\section{Results}

\end{document}















